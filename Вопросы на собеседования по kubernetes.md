***
СОДЕРЖАНИЕ
- [[#Базовые вопросы|Базовые вопросы]]
- [[#Базовые вопросы#Что такое kubernetes?|Что такое kubernetes?]]
- [[#Базовые вопросы#Что такое minikube?|Что такое minikube?]]
- [[#Базовые вопросы#Какую проблему решает kubernetes?|Какую проблему решает kubernetes?]]
- [[#Базовые вопросы#Приведи пример проблемы, которая упрощает работу именно с использованием кубернетеса?|Приведи пример проблемы, которая упрощает работу именно с использованием кубернетеса?]]
- [[#Архтектура кластера|Архтектура кластера]]
- [[#Архтектура кластера#Что такое node (узел, нода)?|Что такое node (узел, нода)?]]
- [[#Архтектура кластера#Опиши архитектуру кубернетес кластера, из чего состоит?|Опиши архитектуру кубернетес кластера, из чего состоит?]]
- [[#Осовные объекты|Осовные объекты]]
- [[#Осовные объекты#Что такое pod?|Что такое pod?]]
- [[#Осовные объекты#Что такое Deployment?|Что такое Deployment?]]
- [[#Осовные объекты#Что такое ReplicaSet?|Что такое ReplicaSet?]]
- [[#Осовные объекты#Что такое StatefulSet?|Что такое StatefulSet?]]
- [[#Осовные объекты#В чем отличие Deployment от StatefulSet ?|В чем отличие Deployment от StatefulSet ?]]
- [[#Осовные объекты#В чем отличие Deployment от ReplicaSet?|В чем отличие Deployment от ReplicaSet?]]
- [[#Осовные объекты#Что такое пробы? Readiness, Liveness, Startup? Какие отличия?|Что такое пробы? Readiness, Liveness, Startup? Какие отличия?]]
- [[#Осовные объекты#Что такое оператор в kubernetes?|Что такое оператор в kubernetes?]]
- [[#Осовные объекты#В чем разница между подом и контейнером?|В чем разница между подом и контейнером?]]
- [[#Осовные объекты#Как создается под? Какие компоненты задействуются при его создании?|Как создается под? Какие компоненты задействуются при его создании?]]
- [[#Осовные объекты#Может ли под запуститься на двух разных узлах?|Может ли под запуститься на двух разных узлах?]]
- [[#Осовные объекты#Что такое Service|Что такое Service]]
- [[#Осовные объекты#Какие типы service бывают?|Какие типы service бывают?]]
- [[#Осовные объекты#Что такое Ingress?|Что такое Ingress?]]
- [[#Осовные объекты#Что такое Job?|Что такое Job?]]
- [[#Осовные объекты#Что такое CronJob?|Что такое CronJob?]]
- [[#Осовные объекты#Что означает версия api (apiVersion)?|Что означает версия api (apiVersion)?]]
- [[#Осовные объекты#Что такое namespace?|Что такое namespace?]]
- [[#Осовные объекты#Что такое Volume?|Что такое Volume?]]
- [[#Осовные объекты#Какие бывают типы файловых хранилищ?|Какие бывают типы файловых хранилищ?]]
- [[#Осовные объекты#Что такое configMap?|Что такое configMap?]]
- [[#Осовные объекты#Что такое Secret?|Что такое Secret?]]
- [[#Осовные объекты#Что такое PersistentVolume, PersistentVolumeClaim?|Что такое PersistentVolume, PersistentVolumeClaim?]]
- [[#Осовные объекты#Что такое nodeSelector, nodeName?|Что такое nodeSelector, nodeName?]]
- [[#Осовные объекты#DaemonSet зачем нужен для чего его обычно используют?|DaemonSet зачем нужен для чего его обычно используют?]]
- [[#Осовные объекты#Что такое Taints, Tolerations?|Что такое Taints, Tolerations?]]
- [[#Осовные объекты#Что такое Requests, Limits?|Что такое Requests, Limits?]]
- [[#Осовные объекты#Affinity, anti-affinity?|Affinity, anti-affinity?]]
- [[#Осовные объекты#Что такое Helm?|Что такое Helm?]]
- [[#Осовные объекты#Что дает Helm в кубе ?|Что дает Helm в кубе ?]]
- [[#Осовные объекты#Если лимит больше чем реквест и нету ресурсов на ноде и выложиться ли такой под?|Если лимит больше чем реквест и нету ресурсов на ноде и выложиться ли такой под?]]
- [[#Осовные объекты#Через что реализованы сети в kubernetes?|Через что реализованы сети в kubernetes?]]
- [[#Осовные объекты#Что произойдет при изменении имейджа? Как будут докатываться изменения?|Что произойдет при изменении имейджа? Как будут докатываться изменения?]]
- [[#Осовные объекты#Что такое headless service ?|Что такое headless service ?]]

***
## Базовые вопросы
***
### Что такое kubernetes?

Это система управления кластерами контейнеров linux. Кубер может запускать и управлять контейнерами на большом количестве хостов. А также имеет возможность это всё размещать и реплицировать.

---
### Что такое minikube?

Локальный кластер для знакомства с кубером, или для проверки каких-либо вещей.

---
### Какую проблему решает kubernetes?


1. Масштабирование и запуск контейнеров на большом количестве хостов
2. Балансировка контейнеров между ними

Также у кубера есть высокоуровневый API для группирования, размещения и балансирования контейнеров.

Какие задачи он решает
- Автоматизация инфраструктуры. Он развертывает приложения, откатывает.
- Масштабирование приложения
- Supervision - контролер, который мониторит состояние кластера. И сравнивает его состояние с требованиями, которые ему описали.
- Service discovery - позволяет сервисы находить в автоматическом режиме.
- Он решает вопросы, которые связаны с логгированием.
- Решает вопросы с мониторингом и сбором метрик
- CI\CD
- Уменьшает vendor lock-in. Мы меньше зависимы от оборудования и провайдеров. Мы тут общаемся с апи кубернетеса. Он как черный ящик, которому мы говорим что делать. И он делает.

---
### Приведи пример проблемы, которая упрощает работу именно с использованием кубернетеса?


Например, у нас есть три машины. На них запущены контейнеры.
И вдруг одна из машин встала с запущенными контейнерами. Или нужно машину перезапустить. И контейнеры нужно переносить.

В итоге нужно будет решать проблемы

1. Контейнеры могут быть связаны, и они должны быть на одной ноде. Значит и перенести нужно на другую ноду их, сохранив эту связанность. Связанность - это использование общих данных. Или активное взаимодействие между собой.
2. Контейнеры не могут “поместиться” на одном узле, и нужно думать а куда вот эти перевести и распределить
3. При возвращении ноды в строй придётся возвращать все контейнеры. Снова нужно делать те же манипуляции.
---
## Архтектура кластера  
---
### Что такое node (узел, нода)?

— это физическая или виртуальная машина, которая является частью кластера. 
Узел может быть мастером и воркером (Master Node & Worker Node).

**Мастер (Master Node)** нужен для размещения управляющих и координирующих элементов в кластере.

**Воркер (Worker Node)** предназначены для рабочей нагрузки. Подики и приложения Как правило отдельные машины на каждое. В миникубе и для мастера, и для воркера одна и та же машина.

---
### Опиши архитектуру кубернетес кластера, из чего состоит?

- Мастер ноды и воркер ноды

- В отказоустойчивом кластере должно быть 3 мастер нод, для достижения кворума, когда большее кол-во мастер нод соглашается с решением для изменения данныхх в etcd.

- Использование нечетного количества мастер-нод помогает избежать ситуации split-brain в Kubernetes. Это явление возникает, когда связь между группами мастер-нод или между всеми нодами одновременно нарушается, и каждая изолированная группаМастерНод/мастер нода начинает считать себя полноценным кластером. Это приводит к проблемам с консистенцией данных в etcd, поскольку разные части кластера могут независимо принимать решения и изменять состояние кластера.

- Воркер нод любое количество - зависит от нагрузки, которую хотим выполнять

**kubectl -** внешний клиент, который обращается к мастер ноде


**Мастер-нода**

**API сервер**

С ним взаимодействуют все компоненты. Он является центральным узлом. Он принимает и обрабатывает запросы по restapi(json и yaml)

**Conroller manager**

Это группа служб, которая следит за состоянием кластера. 
И которая вносит изменения для приведения фактического состояния кластера к желаеому. Мы говорим куберу что мы хотим видеть в конечном статусе.

**Kube-scheduler**

Распределяет рабочую нагрузку по доступным узлам. Распределение выполняется на основании настроек, и на основании текущего использования ресурсов.

Он планирует на каком узле запустить то или иное приложение

**etcd**

Распределенное хранилище. Ключ-значение. Хранит инфу о состоянии кластера.

***Воркер-нода***

1. **kublet**

Служба, которая опрашивает апи сервер на предмет того, какие поды предназначены узлу. (На котором служба находится). И запускает, удаляет их через **container runtime engine**/

Также информирует api сервер о статусе работающих подов на ноде.

2. **kube-proxy**

Компонент, управляющий сетевыми настройками узла. 
По дефолту куб прокси настраивает с помощью iptables правила маршрутизации на эндпоинты сервиса которые можно узнать с помощью `k describe svc/<svc-name>`.
Можно зайти на любой узел и сбросить все правила командой `iptables -F`.
Кубу по дефолту понадобиться 30 секунд что бы понять что праивл нет и восстановить их. 
Либо 10 секунд по дефолту что бы обновить правила маршрутизации при изменении эндпоинтов или сервисов.

Был вопрос на собесе про куб прокси поподробнее.
[можно почитать про kube proxy](https://habr.com/ru/companies/flant/articles/359120/)

3. ** Container runtime** (docker) - компонент, взаимодействующий с контейнером.

---



***
## Осовные объекты
***

### Что такое pod?


Запрос на запуск одного или более контейнеров на одном узле.

Также под это совокупность контейнеров, которые запускаются в ответ на запрос.

Эти контейнеры разделяют доступ к ресурсам типа томов хранилища, и сетевой стек. И каждый под имеет свой собственный внутренний апи

---
### Что такое Deployment?

Переводится как развертывание
Это механизм обновления подов с помощью репликасетов
Он очень похож на репликасет. Но позволяет управляемо обновлять образами подов.

Шаблон деплоймента

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 2 # количество контейнеров
  strategy:
	type: Recreate # стратегия обновления
  selector: # селектор для поиска своих подов
	matchLabels:
	  app: my-app 
  template: # шаблон пода
	metadata:
	  labels:
		app: my-app
	spec:
	  containers:
		- name: nginx
		  image: nginx:1.12
		  ports:
			- containerPort: 80    
```

**Recreate** - стратегия пересоздания при обновлении. В остальном то же самое, что и репликасет.

При модификации деплоймента репликасет удаляется и создается новый с новыми настройками

**RollingUpdate** - он постепенный. То есть поды в него постепенно начинают переезжать(удаляются в одном, в новом репликасете создаются). И это регулируется параметрами maxSurge, maxUnavailable. Лимит превышения и недоступности соотвесттвенно. 
Также репликасеты не удаляются, они будут пустыми. И репликасет это история изменений.

Если создать деплоймент, то можно увидеть следующее

```bash
kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
my-deployment-c4c8f45fc-ltckj   1/1     Running   0          4s
my-deployment-c4c8f45fc-qvp6w   1/1     Running   0          4s    
kubectl get replicasets
NAME                      DESIRED   CURRENT   READY   AGE
my-deployment-c4c8f45fc   2         2         2       17s
```

Создается репликасет с названием деплоймента+уникальныйайди

И поды создаются из названиядеплоймента+уникальныйайдирепликасет+уникальныйайди

Когда создается деплоймент, он внутри себя создает репликасет.

Чтобы обратиться к айпишникам внутри кластера можно попробовать сделать следующее:

```bash
kubectl run --rm -it --image amouat/network-utils test bash
```

---
### Что такое ReplicaSet?


Следующий уровень абстракции над подами.
Она запускает определенное количество подов и гарантирует поддержание данного количества подов. И эти поды могут быть запущены на разных узлах кластера.
В спецификации указываем количество реплик.
Шаблон репликасета

В этом случае будет такой вывод. 
Название будет состоять из основной части и из случайным образом сгенерированной.

```
NAME                  READY   STATUS    RESTARTS   AGE
my-replicaset-28gs2   1/1     Running   0          7m1s
my-replicaset-qg47c   1/1     Running   0          7m1s
```

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
spec:
  replicas: 2 # количество контейнеров
  selector:
	matchLabels:
	  app: my-app # селектор для поиска своих подов. Так репликасет понимает какой под относится к нему, а какие не к нему
  template: # шаблон этого пода. Две реплики по шаблону тут будут
	metadata:
	  labels:
		app: my-app
	spec:
	  containers:
		- image: nginx:1.12
		  name: nginx
		  ports:
			- containerPort: 80
```

И вывод такой реплики:

```bash
kubectl get replicasets -o wide

NAME            DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES       SELECTOR
my-replicaset   2         2         2       10m   nginx        nginx:1.12   app=my-app
```

Видим инфу о копиях в desired
Сколько сейчас запущено в current
Сколько сейчас готово в ready
Возраст в age
Контейнеры
Образ
И метки. По каким меткам репликасет ищет поды.

**ВАЖНО**
При обновлении файла, и образа например - обновится только репликасет. А вот поды будут со старыми значениями пока их не удалю.

Если я хочу чтоб контейнеры менялись, мне нужен деплоймент.    
Репликасет следит только за количеством контейнеров.

---  
### Что такое StatefulSet?

**StatefulSet** - поддерживают состояние приложений за пределами жизненного цикла отдельных модулей pod, 
например для хранилища. Используется для приложений с отслеживанием состояния, 
каждый под будет иметь собственное состояние и будет иметь свой собственный PVC.

- Также statefulset не перебросится на другую ноду. Есть диск, который привязан. И при вымирании ноды, он не переедет в отличие от деплоймента.      Можно использовать NAS ( network attached storage ), nfs сервер или в целом облачную базу данных для того что бы не привязываться к ноде.
- Sigterm statefulset может ждать какое-то время. Потому что поды удаляются там строгом порядке, в обратном от создания. 
- В стейтфулсет прилоежниях нужно сохранить состояние у каждого пода и заменеджерить каждый волюм маунт, что так же может
потребовать больше времени.
- Название подов у statefulset имеет структуру `<имя statefulset>-<номер пода c 0>`

- Statefulset так же исплользуется для баз данных потому что:
- для каждого пода есть темплейт PVC.
- при создании новой реплики происходит клонирования данных с самого первого пода.
- когда происходит write то сначала будет происходить запись на самый первый под,
  а затем эти данные будут копироваться на все остальные.
- удаления подов в statefulset происходит в обратном порядке от создания.

---
### В чем отличие Deployment от StatefulSet ?

**Deployment** - ресурс Kubernetes предназнваенный для развертывания приложения без сохранения состояния. При использовании PVC все реплики будут использовать один и тот же том, и ни один из них не будет иметь собственного состояния.

**StatefulSet**- поддерживают состояние приложений за пределами жизненного цикла отдельных модулей pod, например для хранилища. Используется для приложений с отслеживанием состояния, каждая реплика модуля будет иметь собственное состояние и будет использовать свой собственный том.

Также statefulset не перебросится на другую ноду. Есть диск, который привязан. И при вымирании ноды, он не переедет в отличие от деплоймента.

Sigterm statefulset может ждать какое-то время.

Statefulset для баз используется потому что там есть темплейт PVC.

---
### В чем отличие Deployment от ReplicaSet?


Deployment  упрощает обновление модулей до какой-то определенной или новой версии.
Допустим, у нас есть набор подов из ReplicaSet-A, чтобы выкатить новую версию нужно создать Replicaset-B.
При этом нужно уменьшить Replicaset-A и увеличить Replicaset-B на один шаг несколько раз. То есть выполнить последовательное обновление.

А deployment может это делать автоматически. И просто добавляет еще одну абстракцию.

Иными словами - deployment просто выполняет непрерывное обновление с использованием наборов реплик.

---

### Что такое пробы? Readiness, Liveness, Startup? Какие отличия?

Kubelet использует **Liveness** пробу для проверки, когда перезапустить контейнер. Например, Liveness проба должна поймать блокировку, когда приложение запущено, но не может ничего сделать. В этом случае перезапуск приложения может помочь сделать приложение доступным, несмотря на баги.

Kubelet использует **Readiness** пробы, чтобы узнать, готов ли контейнер принимать траффик. Pod считается готовым, когда все его контейнеры готовы.

Одно из применений такого сигнала - контроль, какие Pod будут использованы в качестве бекенда для сервиса. Пока Pod не в статусе ready, он будет исключен из балансировщиков нагрузки сервиса.

Kubelet использует **Startup** пробы, чтобы понять, когда приложение в контейнере было запущено. Если проба настроена, он блокирует Liveness и Readiness проверки, до того как проба становится успешной, и проверяет, что эта проба не мешает запуску приложения. Это может быть использовано для проверки работоспособности медленно стартующих контейнеров, чтобы избежать убийства kubelet'ом прежде, чем они будут запущены.

Ответ попроще

Начну издалека. Для чего нужны такие механизмы? Дело в том, что запущенный под это не значит запущенное и развернутое приложение. Какая-нибудь джава может по пять минут подниматься.  
Соответственно, нам нужно убедиться в том, что приложение запущено, и что на под можно пускать трафик

1. **readinessProbe** - проверка готовности контейнера. При помощи описанных условий каких-то.

Пример:
```
readinessProbe: # Проверка готовности контейнера
  httpGet: # делаем гет запрос на 80 порт
	path: /
port: 80
  periodSeconds: 2 # периодичнось опроса
  failureThreshold: 3 # сколько допустимо ошибок
  successThreshhold: 1 #  сколько успешных попыток должно быть чтоб под считался готовым к работе
  timeoutSeconds: 1 #таймаут
```

2. **livenessProbe -** предназначена для отслеживания жизнедеятельности контейнера.
Иногда приложение может не работать. Хотя контейнер вроде как крутится. Но приложение не работает или работает плохо.

У него плюс минус те же параметры, что у **readlinessProde**, но у нее есть параметр `initialDelaySeconds` - который определяет через сколько секунд нужно выполнять liveness проверки после запуска контейнера. Также ее стоит сразу выставить на значение 10. Чтобы сразу не было ошибок о якобы нерабочем поде когда он только-только запускается. 

Штука опасная. Может вернуть ошибочно что под недоступен. И кубер пометит под как нерабочий. И это может приводить к постоянному пересозданию подов. 

Некоторые советуют эту пробу вообще не юзать.

---  
### Что такое оператор в kubernetes?

Это контроллер приложения, который позволяет упаковать, развернуть и управлять приложением кубернетеса. Они расширяют функционал апи куба, и автоматически настраивают, создают экземпляры приложений

**другая формулировка**
операторы это по сути под который следит за тем что бы некии ресурсы праивильно разворачивались, 
работали и удалялись, ничего не ломая и тд., так же за счет него можно расширят обычный кластер 
кубернетиса за счет кастомных ресурсов (CRD).

---
### В чем разница между подом и контейнером?

Под это минимальная единица куба. В котором есть контейнеры. 

---

### Как создается под? Какие компоненты задействуются при его создании?

Предварительная информация.
Воркер узлы состоят из трех компонентов.
kubelet - это модуль коммуникации сервера с kubeapi.  
Он сообщает информацию о себе в куб апи. И принимает ее оттуда же.
CRI - container runtime engine - штука, которая непосредственно создает контейнер.
proxy kube server - нужен для взаимодействия узлов между собой, например, когда вычислительные мощности требуют задействовать более, чем один узел.

Первое что задействуется - команда kubectl.
Далее оно попадает на kubeAPI.
KubeAPI - является основным компонентом управления кластером кубера.

Далее kubeapi аутентифицирует и валидирует запрос. Проверит кто делает запрос, и проверит есть ли у запрашиваемого доступ к кластеру.

Далее апи сервер запишет этот под в etcd.
Etcd - это хранилище данных, которое распределено по кластеру, и является "точкой правды" для кластера кубера.
Далее etcd возвращает ответ в апи о том, что под создан. Но по факту пока что еще ничего не создано кроме записи в базе.

Далее в дело вступает планировщик, scheduler. Он следит за нагрузкой которую необходимо создать.
Он определяет на какую ноду можно разместить тот или иной под.
Он периодически опрашивает куб апи на предмет наличия задач. 
Шедулер создает поды на воркер узлах, и смотрит на доступные вычивлительные мощности, место и на ограничения. 
После того, как он определяет где можно создать подходящий под, он сообщает об этом в kubeAPI.

kubeapi обращается в kubelet той ноды, на которую указал шедулер, как на подходящую.
kubelet работает вместе с CRI, который создаст под, в котором работает контейнер.

---

### Может ли под запуститься на двух разных узлах?

Нет. Поскольку есть поле узел. И шедулер назначает какому поду куда ехать.

---
### Что такое Service?

Service - обеспечивает сетевой доступ к поду снаружи.

Это тип ресурса, который заставляет прокси настраиваться на пересылку запросов на набор контейнеров.

У каждого пода айпишник постоянно меняется. И сервис это абстракция, которая предоставляет сетевой доступ к приложению, который работает на группе подов. А также балансирует запросы к этим подам.

Отличается от всех других тем, что не предоставляет свой айпишник

---
### Какие типы Service бывают?

- Без селектора
- ClusterIP - по умолчанию. Сервису выделяется отдельный айпишник внутри кластера. Доступ можно сделать с помощью проксировани.    
- NodePort - выделяется айпи внутри кластера, и на каждом узле выделяется порт из диапазона 30000 - 32767.  То есть указываем на каком порте узлов это работает
- LoadBalancer - использует внешний айпи адрес. И он уже перенаправляет трафик на нод порт и кластер айпи, которые создаются автоматически. Минус в том, что внешний айпи адрес стоит денег. В облаке доступен из коробки а на bare metal нужно что то стороннее, допустим [MetalLB](https://metallb.universe.tf/)
- ExternalName - перенаправление трафика. Через cname в днс кластере.

---
### Что такое Ingress?

Это не тип сервиса, но это абстракция связана с предоставлением доступа к сервисам извне.

Чем-то похоже на нжинкс. Который по локейшнам перенаправляет что-то куда-то.

есть такая особенность что ингресс перенаправляет трафик не на сервис, который мы прописываем в backend.service.name а сразу на ендпоинты, что бы исключить этот лишний хоп.

---
### Что такое Job?

Одноразовая задача. Создает один или несколько подов, и ожидает их успешного завершения.

Если что-то завершается с ошибкой, то джоб бдует запускать новые копии, пока количество успешных выполнений не будет равно заданному.

Типовые примеры:

- Запуск тестов
- Применение миграций базы данных
- Выполнение одноразовых скриптов

- Пример джобы

Успешное выполнение сколько раз нужно чтоб выполнились задачи

Параллельные запуски - контейнеры будут подниматься, одновременно выполняться. Если 1, то друг за дружкой выполняться

**Backofflimit** - максимальное количество попыток. Джоб после этого не будет пытаться что-то делать. И новых контейнеров не создаёт.

**activeDeadlineSecond** - таймлайны для джобы. Больше ничего создаваться не будет. 

**ttlSecondsAfterFinished** - максимальное время жизни завершенного джоба. Нужна для того, чтобы человек мог успеть посмотреть результат. В контейнере логи глянуть например.

**restartPolicy** - регулируем перезапускать или нет. Но самому контейнеру за этим следить не надо. Поэтому Never

---
### Что такое CronJob?

Абстракция, которая автоматически создаёт поды по расписанию. 
Расписание задаётся в крон-формате.

Применение:

- Рассылки писем, уведомлений
- Бэкапы
- Выполнение задач в менее нагруженное время
- Пример кронджобы

---
### Что означает версия api (apiVersion)?

- Ответ
- v1
- v2beta1
- v3aplha1
- extensions/v1/beta1

**Alpha-версии** - отключены по умолчанию. Могут содержать баги. Поддержка может быть прекращена в любое время. И совместоимость с будущими не гарантируется. Не рекомендуется использовать в продакшене.

**Beta-версии** включены по умолчанию. Хорошо протестированы. Поддержка не будет прекращена, но может быть разница в семантике. Если она меняется - будет инструкция по миграции. Не рекомендуется использовать в проде. Но можно в dev для тестирования фич.

**Стабильные версии API** - готовы для продакшена. Совместимы с будущими версиями.

---
### Что такое namespace?
Это пространство имен.
Пространство имен - это способ разделения ресурсов в кластере между пользователями или проектами.

1. Все имена ресурсов должны быть уникальными в пределах одного и того же пространства имён.
2. Каждый ресурс может быть только в одном пространстве имен
3. Пространства имен не могут быть вложенными.

По умолчанию существует три пространства имен

- **Default** - пространство имён по умолчанию для объектов. Без какого-либо иного пространства имён
- **kube-system** - пространство имён для системных служебных объектов кубернетеса.
- **kube-public** - создаваемое автоматически пространство имён, которое доступно для чтения пользователями. Данное пространство имён обычно используется кластером, если ресурсы должны быть общедоступными.

---
### Что такое Volume?

Volume - это абстракция файлового хранилища.

Решает следующие основные проблемы:

- Файловая ситема контейнера существует только до его удаления или перезапуска
- Некоторым контейнерам нужно общее пространство для хранения файлов, или для обращения к конфигурационным файлам.
- Изолирует приложение от технологий хранения данных

Живет только с подом

---   
### Какие бывают типы файловых хранилищ?

**emptyDir** — каталог, который создается до пода, и и живет пока под не будет удален. 
- Все контейнеры в нем могут читать данные из этого volume. 
- В качестве системы хранения - используется файловая система узла.

**hostPath**
- В hostPath папка на узле уже существует, и подключается к подам. Данные не пропадают после удаления пода.

В случае с emptyDir создается папка, подмонтируется. Если под падает - данные пропадают.

- Используется для случаев, когда под отслеживает состояние узла. Предоставляется доступ к системным каталогам

Также там есть ключи DirectoryOrCreate и FileOrCreate - они нужны для задачи кубернетесу проверить наличие папки, и потом обращение к файлу.

---
### Что такое configMap?

API объект, который используется для хранение неконфиценциальных параметров типа ключ значение.

Это этакое хранилище параметров

Поды могут использовать значения оттуда несколькими способами:

- Конф файлы, подключаемые как volumes
- Через Переменные окружения
- Через Параметры запуска контейнеров
- Пример использования

В volumes на уровня деплоймента в спецификации указывается конфиг мап, и имя конфиг мапа. 

А в спеке контейнера указываем имя уже этого вольюма

---
### Что такое Secret?
— объект, который содержит конфиденциально важное значение. (пароль, токен, ключ).
- Хранение важных параметров в виде секретов более безопасно и гибко, чем включение их в конфигурацию пода или в образы контейнеров.
- Для использования секрета поду необходимо указать его.

Секрет может использоваться следующими способами:
- Файл подключенный через volume контейнера.
- Значение переменной окружения
- kubelet использует секреты для подключения к апи серверу, для загрзки образов из докер регистри

---
### Что такое PersistentVolume, PersistentVolumeClaim?

**PersistentVolume (PV)**  - такой же ресурс кластера, как и узел. Он предоставляет не вычислительные русерсы, а тома дисковые. Примеры: NFS, RBD, CephFS и другие. В рамках PV впоследствии можно выделять подам место для хранения данных.

**PersistenceVolumeClaim(PVC)**  - запрос к PV на выделение места под хранение данных. Это аналог создания пода на узле. Поды могут запрашивать определенные ресурсы узла, то же делает и PVC. Сколько ему места нужно, типы доступа. Основные параметры запроса:

- Объем места
- Тип доступа

**Типы доступа у PVC**:

- **ReadWriteOnce** - том может быть смонтирован на чтение и запись к одному поду
- **ReadOnlyMany** - том может быть смонтирован на много подов в режиме реального времени
- **ReadWriteMany** - том может быть смонтирован к множеству подов в режиме чтения и записи
- Схема статической и динамической

1) **Статическое**.

- Сторадж это хранилище. Диск, облачный ресурс.

Для того, чтобы к поду подмонтировалась часть пространства, администратор должен создать набор PersistanceVolume. Допустим диск на 500гб, админ нарезает его с помощью PersistanceVolume на блоки. По 10, 20, 50 гб. И так далее. Короче набор таких PV.

Когда поду нужно место PVC делает заявку о том, что ему нужен диск определенного размера с определенным типом доступа. И кубер подбирает подходящие PV. И не всегда запрос совпадает с тем, что есть. Допустим, у нас нарезано по 100гб PV, а PVC запрашивает на 5 гб, то кубер отдаст на 100. Других вариантов нету просто. Нужно оптимиально их разбивать.

Эту проблему решает динамическая схема создания.

2) **Динамическое**

Здесь плагин сам формируует PV при наличии запроса на него.

Админ создает сторадж класс, где описывается сторадж, который подключил к кластеру. Ну и с помощью плагина в ответ на запрос pvc кубер создаст pv нужного размера, и предоставит для пода.

---
### Что такое nodeSelector, nodeName?

Это тип балансировщика.

В спецификации пода указываются метки узлов (nodeSelector), на которых мы хотим его запустить или указывается определенный узел (nodeName)

Например

```
# Данный под будет назначен на узел, где есть специфический тип диска
nodeSelector:
  disktype: ssd

# можем указать название узла куда под должен быть направлен
nodeName: kube-01
```

---
### DaemonSet зачем нужен для чего его обычно используют?

У него широкое распространение. Нужен для сбора логов по разным нодам. Логи как-то надо собирать. Сертификаты нод. Смотреть через демонсет можно такие штуки.
Еще DaemonSet не нужен шедулер ведь для сразу резервируются ресурсы на каждой ноде

---
### Что такое Taints, Tolerations?

Можно настроить список блокировок (taints). Или как зараза. И если в спецификации пода не указана сопротивляемость к блокировкам(tolerations), то он не сможет попасть на определенный узел.

То есть мы говорим поду где НЕ запускаться.

Виды блокировок:

**NoSchedule**

Не размещать на узле без соответсвующих tolerations, поды которые уже работают на узле, не будут затронуты.

**PreferNoSchedule**

Предпочитать не размещать на узле поды без соответствующих tolerations, но  не требовать этого.

**NoExecute**

Не размещать на узле поды без соответствующих tolerations. все поды которые уже есть на 
ноде с таким taint и не будут иметь соответствующие tolerations, будут мувнуты с нее на другю ноду.

---
### Что такое Requests, Limits?

В спецификации пода мы можем указать плановое потребление ресурсов(requests) и лимиты использования ресурсов(limits). Нужный узел подбирается автоматически с учетом текущей нагрузки.

cpu: 50m - m -milicpu. 

limits - контейнер будет убить. Непредсказумая настройкая.

Есть QoS классы подов

- Guaranteed (limits = requests) //Под не может превысить свои ресурсы. Сколько запросил столько и получил
- Burstable (limits > requests) кто-то запросил 50 цпу, но иногда превышает его. И часть контейнеров при нагрузке будут переезжать
- BestEffort (не указаны limits и requests) - не указываем лимиты реквесты. Первыми будут переезжать эти

Можно увидеть через describe пода

---
### Affinity, anti-affinity?

В спецификации пода указываются требования и пожелания к узлам, а также к уже существующим подам.

Как нод селектор. Но можно заказать условия. Типа метка может быть определенных типов, перечисленных.

**example**:

```yaml
affinity:
	nodeAffinity:
	  requiredDuringSchedulingIgnoredDuringExecution:
	  #параметр выше отвечает за то, что эти правила бдуут применяться при планировки а не при работе нод и тд
		nodeSelectorTerms:
		- matchExpressions: #disktype должен быть не hdd
		  - key: disktype
			operator: NotIn
			values:
			- hdd
		- matchExpressions: #RAM памяти должено быть больше чем 8гб на ноде 
		  - key: memory
			operator: Gt
			values:
			- 8Gi
		- matchExpressions: #gpu должн обыть на ноде
		  - key: gpu
			operator: Exists
  podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - weight: 100 #позволяет задать вес правилу и там самым можно управлять их приоритетностью
	podAntiAffinityTerm:
	  labelSelector:
		matchExpressions: #в целом это правило указываеьт что бы поды с меткой app = web-app были на разных хостах
		- key: app
		  operator: In
		  values:
		  - web-app
	  #topologyKey позволяет указать на каком уровне будут применяться правила, здесь на уровне  хоста
	  topologyKey: kubernetes.io/hostname 
  podAffinity:
	requiredDuringSchedulingIgnoredDuringExecution:
	- podAffinityTerm:
		labelSelector: 
		  matchExpressions: #лейбл security должен быть = s1 на поде
		  - key: security
			operator: In
			values:
			- s1
		topologyKey: kubernetes.io/hostname
```

---
### Что такое Helm?

Пакетный менеджер для кубера.
Шаблонизатор для управления

Пакет в helm это набор yml и tpl файлов.

*.tpl файлы нужны для определения каких то функций
который будут возвращать какие то параметры либо имена и тд
в файле *.tpl ты с помощью ключевого слова define определяешь функцию
а в самом хелм чарте с помощью include, вставляешь ее результат.
Обычно используется для генерации labels, полных имен и тд.

---
### Что дает Helm в кубе ?

1. Версионирования.
2. Шаблонизирование.
3. Удобство развертывания инфраструктуры на разные контура.
4. Более подконтрольная работа с конкретной разворачиваемой инфраструктурой.
5. Возможность подтягивать зависимости. Допустим  если у тебя хелм чарт 
   зависит от другого хелм чарта, ты можешьб либо в самом Chart.yml 
   файле в директиве dependency прописать зависимотси, либо 
   в отдельном файле requirements.yml прописать все зависимости
   и перед раскаткой основного хелм чарта поднятнуть все зависимости командой 
   `helm dependency update`.
---
### Если лимит больше чем реквест и нету ресурсов на ноде и выложиться ли такой под?

да, ведь по реквесту под может развернуться, но как только под будет требовать ресурсов больше чем есть на ноде то под убьеться (этот процесс называется оверкомит, и как в линуксе есть что каждый процесс может запуститься но потом убьется если будет тербовать много ресурсов)

---
### Через что реализованы сети в kubernetes? 

Кубер для реализации сетей использует различные плагины CNI (Container Networking Interface). 
Наиболее известные **flannel** и **calico**.

Пример получения IP используя *flannel*:
Kube-controller-manager каждому узлу присваивает podCIDR. Pod'ы каждого узла получают IP-адреса из пространства адресов в выделенном диапазоне podCIDR. Поскольку podCIDR'ы узлов не пересекаются, все pod'ы получают уникальные IP-адреса.
Во время старта агент сетевого провайдера генерирует конфиг CNI. Когда pod планируется на узел, kubelet вызывает CRI-плагин для его создания. Далее, если используется containerd, плагин Containerd CRI вызывает CNI-плагин, указанный в конфиге CNI, для настройки сети pod'а. 
В результате pod получает IP-адрес.
![flannel-netowrk](kubernetes-flannel-network.png)

Плюсы использования *calico*
* поддерживает network policies

---
### Что произойдет при изменении имейджа? Как будут докатываться изменения?

В общем случае создастся количество подов с определенным заданым количеством. Потом будут уходить “старые" поды. Это контролируется политикой обновлений.

И когда хелсчеки прошли по новым.

---
### Что такое headless Service ? 
Это такой сервис, который привязывается к какому-то ворклоаду и не имеет как такового IP адреса, а имеет DNS запись. 
При ответе он подставляет не свой IP адрес, а все IP адреса, которые лейблом к этому хедлесс сервису.

Как я понял, это полезно, когда тебе нужно у какой-то группы подов раскрыть их имена и обращаться к ним 
напрямую без балансировки нагрузки, что может помочь не взаимодействовать с мертвыми подами.

в ямле это сервисе где указывается `clusterIP: None` и может выглядить как:

```yaml 
apiVersion: v1
kind: Service
metadata:
  name: my-headless-service
spec:
  selector:
	app: web-app
  clusterIP: None
  ports:
	port: 80
	targetPort: 8080
```

допустим как может выглядеть пример взаимодействия через headless servic  и черещз обычнй сервис

```bash
kubectl get pods -l app=web-app -o jsonpath='{.items[*].metadata.name}' #получаем список подов

curl http://<pod-name>.my-headless-service.default.svc.cluster.local #делам запрос на конкретный под

curl http://my-headless-service.default.svc.cluster.local:80 #может сделать запрос просто на сервис
```

через обычный сервис
```bash
curl http://common-service
```
---

#devops #k8s 